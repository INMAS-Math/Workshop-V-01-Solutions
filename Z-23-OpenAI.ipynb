{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d3a007",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/inmas.png\" width=130x align='right' />\n",
    "\n",
    "# Exercises 23 - OpenAI Application Programming Interface\n",
    "    \n",
    "This notebook requires that you concurrently use a terminal. Use virtual desktops if possible (e.g., Windows Key+Ctl+arrows).\n",
    "\n",
    "### Prerequisite\n",
    "Notebook 23\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bded63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave, struct, os, time\n",
    "from openai import OpenAI                        # The API to OpenAI\n",
    "from pvrecorder import PvRecorder                # To record our voice\n",
    "from playsound import playsound                  # To play the generated speech\n",
    "from IPython.display import Image, display       # To show generated images\n",
    "\n",
    "# To silence deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67a174",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Experiment with different voices and cues for the personality of the system and for the image generation in the last version of the ChatBot class. You must have noticed that chatgpt and DALL-E have an *unconscious* bias toward male professors: while the gender was never specified, it always returns male images.\n",
    "- Change the voice to a female or neutral voice (only *onyx* and *echo* are male voices)\n",
    "- Change the cues to chatgpt (`system`) so that both the personality and the generated image represent a female professor\n",
    "\n",
    "Voice samples are [here](https://platform.openai.com/docs/guides/text-to-speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7d871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25fb6a02",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "<font face=\"verdana\" style=\"font-size:20px\" color=\"blue\"> Solution 1. </font>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a348f93",
   "metadata": {},
   "source": [
    "- Add a specification that the professor is female to:\n",
    "    - the text in the `system` item for answering the question\n",
    "    - the text in the `system` item for describing the image as a prompt to the image generator\n",
    "- Change to voice to your preferred female voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24804bd",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Use a call to 'gpt-4o' to generate code in python to play Hangman. Copy the response message to a cell and run the program to see if it behaves as it should. You will need an API key and some credits on OpenAI to accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af7f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecf1431",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "<font face=\"verdana\" style=\"font-size:20px\" color=\"blue\"> Solution 2. </font>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c429b",
   "metadata": {},
   "source": [
    "Solution requires an API key, a client, and a call like below, with the message describing the Hangman problem and the seeked solution in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098cea4",
   "metadata": {},
   "source": [
    "### 2.*\n",
    "This exercise further develops the class ChatBot that was presented in the Notebook. We reproduce it here for your convenience:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbcded7",
   "metadata": {},
   "source": [
    "```python\n",
    "class ChatBot:\n",
    "    '''A class to interact with OpenAI API and keep track of context.'''\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = [{'role': 'system', 'content': 'You are math professor with a dark sense of humor'}]\n",
    "\n",
    "    def chat(self, question):\n",
    "        self.context.append({'role': 'user', 'content': question})\n",
    "        response = self.client.chat.completions.create(model=self.model, messages=self.context)\n",
    "        response_content = response.choices[0].message.content\n",
    "        self.context.append({'role': 'assistant', 'content': response_content})\n",
    "        self.print_chat()\n",
    "        self.speak(response_content, len(self.context)/2)\n",
    "\n",
    "    def speak(self, message, index=0):\n",
    "        speech_file = './_speech_%03d.mp3'%index\n",
    "        response = client.audio.speech.create(model='tts-1-hd', voice='echo', input=message)\n",
    "        if os.path.exists(speech_file):\n",
    "            os.remove(speech_file)\n",
    "        response.stream_to_file(speech_file)\n",
    "        time.sleep(1)\n",
    "        playsound(speech_file)   # playsound 1.2.2\n",
    "        '''\n",
    "        done = False\n",
    "        while not done:       # Use this loop instead to get around a bug in `playsound` 1.3.0\n",
    "            try:\n",
    "                playsound(speech_file)\n",
    "                done = True\n",
    "            except:\n",
    "                continue\n",
    "        '''\n",
    "        \n",
    "    def print_chat(self):\n",
    "        for message in self.context:\n",
    "            if message['role'] == 'user':\n",
    "                print('USER: %s' % message['content'])\n",
    "            elif message['role'] == 'assistant':\n",
    "                print('BOT: %s' % message['content'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef977a4",
   "metadata": {},
   "source": [
    "We would like to speak to our Bot and convert our speech to text using the *Whisper* model in OpenAI. \n",
    "\n",
    "The first step is to record our voice using the following code. It records until a time is over. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c970a",
   "metadata": {},
   "source": [
    "```python\n",
    "index = 0\n",
    "rec_time = 10\n",
    "audiofile = './_audio_recording%03d.wav'%index\n",
    "recorder = PvRecorder(device_index=-1, frame_length=512)\n",
    "audio = []\n",
    "\n",
    "recorder.start()\n",
    "print('Audio recording started: you have %d seconds to ask your question...'%rec_time)\n",
    "t_end = time.time() + rec_time\n",
    "while time.time() < t_end:\n",
    "    frame = recorder.read()\n",
    "    audio.extend(frame)\n",
    "\n",
    "recorder.delete()\n",
    "\n",
    "print('Audio recording stopped.')\n",
    "with wave.open(audiofile, 'w') as f:\n",
    "    f.setparams((1, 2, 16000, 512, 'NONE', 'not compressed')) # 1=mono, 2=16bit audio, 16KHz, compression, name\n",
    "    f.writeframes(struct.pack('h'*len(audio), *audio))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb366928",
   "metadata": {},
   "source": [
    "If you have multiple sound devices on your computer you can list them an choose or use `-1` which picks the default device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = PvRecorder.get_available_devices()\n",
    "print('Devices:', devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed8970",
   "metadata": {},
   "source": [
    "We now use the audiofile which contains our recorded voice and pass it to *Whisper* for transcription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(audiofile) as f:\n",
    "    transcript = client.audio.transcription.create(model='whisper-1', file=f)\n",
    "\n",
    "transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca9afb",
   "metadata": {},
   "source": [
    "Using the content of the last cell for a method called `transcribe(self, audiofile)` which returns the transcribed text to be passed as a message, and the content of the previous one in a method called `record_audio(self, rec_time, index=0)` **which returns the path of the audio file**,  integrate the last two cells in the ChatBot class. One additional method should be added:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215bb8bb",
   "metadata": {},
   "source": [
    "```python\n",
    "def voicechat(self, rec_time):\n",
    "    recorded_audio = self.record_audio(rec_time, index=len(self.context)/2)   # Use response count for unique filename\n",
    "    message = self.transcribe(recorded_audio)\n",
    "    self.chat(message)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41f496",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "<font face=\"verdana\" style=\"font-size:20px\" color=\"blue\"> Solution 2. </font>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62870d3",
   "metadata": {},
   "source": [
    "Solution is to glue all code in a single cell under the class ChatBot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    '''A class to interact with OpenAI API and keep track of context.'''\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = [{'role': 'system', 'content': 'You are math professor with a dark sense of humor'}]\n",
    "\n",
    "    def chat(self, question):\n",
    "        self.context.append({'role': 'user', 'content': question})\n",
    "        response = self.client.chat.completions.create(model=self.model, messages=self.context)\n",
    "        response_content = response.choices[0].message.content\n",
    "        self.context.append({'role': 'assistant', 'content': response_content})\n",
    "        self.print_chat()\n",
    "        self.speak(response_content, len(self.context)/2)\n",
    "\n",
    "    def speak(self, message, index=0):\n",
    "        speech_file = './_speech_%03d.mp3'%index\n",
    "        response = client.audio.speech.create(model='tts-1-hd', voice='echo', input=message, response_format='mp3')\n",
    "        if os.path.exists(speech_file):\n",
    "            os.remove(speech_file)\n",
    "        response.stream_to_file(speech_file)\n",
    "        time.sleep(1)\n",
    "        playsound(speech_file)\n",
    "        '''\n",
    "        done = False\n",
    "        while not done:       # Use this loop instead to get around a bug in `playsound` 1.3.0\n",
    "            try:\n",
    "                playsound(speech_file)\n",
    "                done = True\n",
    "            except:\n",
    "                continue\n",
    "        '''\n",
    "\n",
    "    def print_chat(self):\n",
    "        for message in self.context:\n",
    "            if message['role'] == 'user':\n",
    "                print('USER: %s' % message['content'])\n",
    "            elif message['role'] == 'assistant':\n",
    "                print('BOT: %s' % message['content'])\n",
    "\n",
    "    def voicechat(self, rec_time):\n",
    "        recorded_audio = self.record_audio(rec_time, index=len(self.context)/2)   # Use response count for unique filename\n",
    "        message = self.transcribe(recorded_audio)\n",
    "        self.chat(message)\n",
    "    \n",
    "    def transcribe(self, audiofile):\n",
    "        with open(audiofile, 'rb') as f:\n",
    "            transcript = client.audio.transcriptions.create(model='whisper-1', file=f)\n",
    "\n",
    "        print('Your question:', transcript.text)\n",
    "        return transcript.text\n",
    "\n",
    "    def record_audio(self, rec_time, index=0):\n",
    "        audiofile = './_audio_recording%03d.wav'%index\n",
    "        recorder = PvRecorder(device_index=1, frame_length=512)\n",
    "        audio = []\n",
    "\n",
    "        recorder.start()\n",
    "        print('Audio recording started: you have %d seconds to ask your question...'%rec_time)\n",
    "        t_end = time.time() + rec_time\n",
    "        while time.time() < t_end:\n",
    "            frame = recorder.read()\n",
    "            audio.extend(frame)\n",
    "\n",
    "        recorder.delete()\n",
    "        \n",
    "        print('Audio recording stopped.')\n",
    "        with wave.open(audiofile, 'wb') as f:\n",
    "            f.setparams((1, 2, 16000, 512, 'NONE', 'not compressed')) # 1=mono, 2=16bit audio, 16KHz, compression, name\n",
    "            f.writeframes(struct.pack('h'*len(audio), *audio))\n",
    "        \n",
    "        return audiofile\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafefa7",
   "metadata": {},
   "source": [
    "Enter you API key here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "APIkey = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ab72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=APIkey)\n",
    "chatbot = ChatBot(client, 'gpt-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e0556",
   "metadata": {},
   "source": [
    "Repeat as many times as you need to keep context. Ask your questions by (re-)running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.voicechat(3)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:chatgpt]",
   "language": "python",
   "name": "conda-env-chatgpt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
